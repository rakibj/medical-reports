{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ff9480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "from dotenv import load_dotenv\n",
    "import os, re, math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "from supabase import create_client, Client\n",
    "from postgrest import APIError \n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb8466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "endpoint_url=os.getenv(\"B2_ENDPOINT_URL\")\n",
    "aws_access_key_id=os.getenv(\"B2_KEY_ID\")\n",
    "aws_secret_access_key=os.getenv(\"B2_APP_KEY\")\n",
    "bucket_name = os.getenv(\"B2_BUCKET_NAME\")\n",
    "user_id   = os.getenv(\"B2_USER_ID\")\n",
    "region_name  = os.getenv(\"B2_REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97225af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=endpoint_url,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"virtual\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "665ed539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: generate sensible keys for uploaded files\n",
    "def make_report_key(user_id: str, report_id: str, filename: str, subfolder: str = \"source\") -> str:\n",
    "    \"\"\"\n",
    "    Build an object key like:\n",
    "    acct/{user_id}/reports/{report_id}/{subfolder}/{filename}\n",
    "    \"\"\"\n",
    "    return f\"acct/{user_id}/reports/{report_id}/{subfolder}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b7702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presigned_url(user_id: str, report_id: str, filename: str, expires_in: int = 900) -> str:\n",
    "    \"\"\"\n",
    "    Generate a presigned URL so the client can download/view the file directly.\n",
    "    \"\"\"\n",
    "    key = make_report_key(user_id, report_id, filename)\n",
    "    url = s3.generate_presigned_url(\n",
    "        \"get_object\",\n",
    "        Params={\"Bucket\": bucket_name, \"Key\": key},\n",
    "        ExpiresIn=expires_in\n",
    "    )\n",
    "    print(f\"🔗 Presigned URL (valid {expires_in}s): {url}\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5685c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_content_type(filename: str) -> str:\n",
    "    ctype, _ = mimetypes.guess_type(filename)\n",
    "    if ctype:\n",
    "        return ctype\n",
    "    ext = Path(filename).suffix.lower()\n",
    "    return {\n",
    "        \".jpg\": \"image/jpeg\",\n",
    "        \".jpeg\": \"image/jpeg\",\n",
    "        \".png\": \"image/png\",\n",
    "        \".gif\": \"image/gif\",\n",
    "        \".webp\": \"image/webp\",\n",
    "        \".pdf\": \"application/pdf\",\n",
    "    }.get(ext, \"application/octet-stream\")\n",
    "\n",
    "def infer_extension(local_path: str) -> str:\n",
    "    ext = Path(local_path).suffix.lstrip(\".\") \n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0deb00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "supabase_service_role_key = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "supabase_default_account_id = os.getenv(\"SUPABASE_DEFAULT_ACCOUNT_ID\")\n",
    "\n",
    "supabase: Client = create_client(supabase_url, supabase_service_role_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e219dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_report(user_id: str, local_path: str, report_id: str, filename: str, content_type: str) -> str:\n",
    "    key = make_report_key(user_id, report_id, filename, subfolder=\"source\")\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=key,\n",
    "            Body=f,\n",
    "            ContentType=content_type,\n",
    "            Metadata={\"report-id\": report_id},\n",
    "        )\n",
    "    print(f\"✅ Uploaded → {key}\")\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "637fd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def pdf_to_images_pypdfium2(pdf_path: str, dpi: int = 200):\n",
    "    pdf = pdfium.PdfDocument(str(pdf_path))\n",
    "    scale = dpi / 72.0  # PDF base is 72 dpi\n",
    "    images = []\n",
    "    for i in range(len(pdf)):\n",
    "        page = pdf[i]\n",
    "        pil = page.render(scale=scale).to_pil()   # PIL.Image\n",
    "        images.append(pil.convert(\"RGB\"))\n",
    "    pdf.close()\n",
    "    return images\n",
    "\n",
    "def encode_image_to_base64(img: Image.Image) -> str:\n",
    "    \"\"\"Convert PIL image to base64 string.\"\"\"\n",
    "    from io import BytesIO\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def ocr_page(image: Image.Image) -> str:\n",
    "    \"\"\"Send one image to OpenAI for OCR.\"\"\"\n",
    "    b64 = encode_image_to_base64(image)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # cheap + handles handwriting\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an OCR engine. Output the text exactly as written.\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Extract the text from this document page. Only return the extracted text and nothing else\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}},\n",
    "            ]},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def ocr_file(file_path: str) -> str:\n",
    "    \"\"\"Auto-detect type and OCR accordingly.\"\"\"\n",
    "    path = Path(file_path)\n",
    "    ext = path.suffix.lower()\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        #pages = convert_from_path(file_path, dpi=200)\n",
    "        pages = pdf_to_images_pypdfium2(file_path, dpi=200)\n",
    "        for i, page in enumerate(pages, start=1):\n",
    "            print(f\"OCR page {i}/{len(pages)}...\")\n",
    "            texts.append(ocr_page(page))\n",
    "    else:\n",
    "        img = Image.open(file_path).convert(\"RGB\")\n",
    "        texts.append(ocr_page(img))\n",
    "\n",
    "    return \"\\n\\n--- PAGE BREAK ---\\n\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d101d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_database_report(account_id: str, report_id: str, filename: str, mime_type: str, size_bytes: int) -> None:\n",
    "    try:\n",
    "        payload = {\n",
    "        \"id\": report_id,\n",
    "        \"account_id\": account_id,\n",
    "        \"filename\": filename,\n",
    "        \"mime_type\": mime_type,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"upload_status\": \"uploaded\",\n",
    "        \"ocr_status\": \"queued\",\n",
    "        }\n",
    "        supabase.table(\"reports\").insert(payload).execute()\n",
    "        print(f\"🗄️  Report metadata added to database (ID: {report_id})\")\n",
    "    except APIError as e:\n",
    "        print(\"❌ Failed to add report metadata to database:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0745444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect vector store\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # 1536 dims\n",
    "\n",
    "# ---- tiny splitter (roughly 700–900 tokens per chunk if English-like)\n",
    "def simple_chunk(text: str, max_chars: int = 3500, overlap: int = 200) -> List[str]:\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        end = min(i + max_chars, len(text))\n",
    "        # try to break at sentence boundary near the end\n",
    "        cut = text.rfind(\". \", i, end)\n",
    "        if cut == -1 or cut < i + int(max_chars * 0.6):\n",
    "            cut = end\n",
    "        else:\n",
    "            cut += 1  # include the period\n",
    "        chunks.append(text[i:cut].strip())\n",
    "        i = max(cut - overlap, cut)  # ensure progress if overlap > chunk\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "\n",
    "def upsert_chunks(\n",
    "    account_id: str, report_id: str, page_no: int | None, chunks: List[str]\n",
    ") -> None:\n",
    "    if not chunks:\n",
    "        return\n",
    "    embs = embed_texts(chunks)\n",
    "    rows = []\n",
    "    for idx, (c, e) in enumerate(zip(chunks, embs)):\n",
    "        rows.append({\n",
    "            \"account_id\": account_id,\n",
    "            \"report_id\": report_id,\n",
    "            \"page_no\": page_no,\n",
    "            \"chunk_no\": idx,\n",
    "            \"content\": c,\n",
    "            \"content_tokens\": None,   # fill if you count tokens later\n",
    "            \"embedding\": e,\n",
    "        })\n",
    "    # Supabase can insert lists of JSON rows directly\n",
    "    supabase.table(\"report_chunks\").upsert(rows, on_conflict=\"report_id,chunk_no\").execute()\n",
    "    \n",
    "\n",
    "# Example: from your OCR text (whole doc) → chunk → embed → store\n",
    "def index_ocr_text(account_id: str, report_id: str, full_text: str, page_map: List[Tuple[int, str]] | None = None):\n",
    "    \"\"\"\n",
    "    If you have per-page OCR, pass page_map=[(1, text1), (2, text2)...].\n",
    "    Otherwise pass full_text and leave page_map=None.\n",
    "    \"\"\"\n",
    "    if page_map:\n",
    "        for page_no, page_text in page_map:\n",
    "            chunks = simple_chunk(page_text)\n",
    "            upsert_chunks(account_id, report_id, page_no, chunks)\n",
    "    else:\n",
    "        chunks = simple_chunk(full_text)\n",
    "        upsert_chunks(account_id, report_id, None, chunks)\n",
    "    print(\"✅ Upserting embeddings to database completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "598df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_upload_report(account_id: str, file_path: str):\n",
    "    \n",
    "    path = Path(file_path)\n",
    "    report_id = str(uuid.uuid4())\n",
    "    filename = \"file.\" + infer_extension(file_path)\n",
    "    mime_type = infer_content_type(filename)\n",
    "    size_bytes = path.stat().st_size\n",
    "\n",
    "    add_database_report(account_id, report_id, filename, mime_type, size_bytes)\n",
    "    upload_report(user_id=user_id, local_path=file_path, report_id=report_id, filename=filename, content_type=mime_type)\n",
    "    full_text = ocr_file(file_path)\n",
    "    index_ocr_text(supabase_default_account_id, report_id, full_text)\n",
    "\n",
    "    return report_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8328c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️  Report metadata added to database (ID: fe41627e-82fe-4121-8fb0-49d24b9fb130)\n",
      "✅ Uploaded → acct/tester/reports/fe41627e-82fe-4121-8fb0-49d24b9fb130/source/file.pdf\n",
      "OCR page 1/1...\n",
      "✅ Upserting embeddings to database completed\n",
      "🔗 Presigned URL (valid 600s): https://medical-reports.s3.us-east-005.backblazeb2.com/acct/tester/reports/fe41627e-82fe-4121-8fb0-49d24b9fb130/source/file.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=005704149b704450000000002%2F20250920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250920T055037Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=629ac37e287e8984089c7184700ff9bdb70eda7aaec0f271d4fc8e7c4a9246a4\n",
      "Download from: https://medical-reports.s3.us-east-005.backblazeb2.com/acct/tester/reports/fe41627e-82fe-4121-8fb0-49d24b9fb130/source/file.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=005704149b704450000000002%2F20250920%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250920T055037Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=629ac37e287e8984089c7184700ff9bdb70eda7aaec0f271d4fc8e7c4a9246a4\n"
     ]
    }
   ],
   "source": [
    "img_path = BASE_DIR / \"resources\" / \"sample_report.png\"\n",
    "pdf_path = BASE_DIR / \"resources\" / \"sample_report.pdf\"\n",
    "report_id = prepare_and_upload_report(supabase_default_account_id, pdf_path)\n",
    "url = get_presigned_url(user_id, report_id, \"file.\" + infer_extension(pdf_path), expires_in=600)  # 10 minutes\n",
    "print(\"Download from:\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aff177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(q: str) -> list[float]:\n",
    "    return client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\", input=[q]\n",
    "    ).data[0].embedding\n",
    "\n",
    "def search_chunks(sb, account_id: str, query: str, k: int = 5, report_id: str | None = None):\n",
    "    q_emb = embed_query(query)\n",
    "    resp = sb.rpc(\n",
    "        \"search_report_chunks_json_simple\",\n",
    "        {\"p_account_id\": account_id, \"p_query\": q_emb, \"p_limit\": k, \"p_report_id\": report_id}\n",
    "    ).execute()\n",
    "    return resp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04247a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_from_embeddings(\n",
    "    supabase,\n",
    "    account_id: str,\n",
    "    query: str,\n",
    "    report_id: Optional[str],\n",
    "    *,\n",
    "    k: int = 5,\n",
    "    sim_threshold: float = 0.10,     # tweak: 0.30 (weak) · 0.50 (medium) · 0.70 (strong)\n",
    "    max_snippet: int = 1200,\n",
    "    max_total_chars: int = 4000,     # safety cap on total context\n",
    "    min_hits: int = 1,               # require at least this many hits above threshold\n",
    ") -> str:\n",
    "    rows: List[Dict] = search_chunks(supabase, account_id, query, k=k, report_id=report_id) or []\n",
    "\n",
    "    # keep only rows with similarity >= threshold\n",
    "    good: List[Dict] = []\n",
    "    for r in rows:\n",
    "        try:\n",
    "            sim = float(r.get(\"similarity\", 0.0))\n",
    "        except (TypeError, ValueError):\n",
    "            sim = 0.0\n",
    "        if sim >= sim_threshold:\n",
    "            good.append(r)\n",
    "\n",
    "    if len(good) < min_hits:\n",
    "        return \"\"  # << no usable context\n",
    "\n",
    "    parts: List[str] = []\n",
    "    for r in good:\n",
    "        sim = float(r.get(\"similarity\", 0.0))\n",
    "        page = r.get(\"page_no\")\n",
    "        page_str = str(page) if page is not None else \"?\"\n",
    "        snippet = (r.get(\"content\") or \"\").strip()\n",
    "        if len(snippet) > max_snippet:\n",
    "            snippet = snippet[:max_snippet] + \" …\"\n",
    "        parts.append(f\"[page {page_str} | sim {sim:.3f}] {snippet}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(parts)\n",
    "    if len(context) > max_total_chars:\n",
    "        context = context[:max_total_chars] + \" …\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4253cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[page ? | sim 0.205] ``` MyPrime Simpletree Anarkali, Holding No. 89, Plot No. 03, Block: CWS(A), Gulshan Avenue, Dhaka North City Corporation, Bangladesh. TRANSACTION RECEIPT Other Bank Transfer(NPSB) Source Account/Card 2125214023710 Amount BDT 3892.00 Transaction Date Time 2025-09-17 09:45:16 Narration September 2025 due bill Transaction Info To Account/Card No. 1114112000000042 Reference No. 526009452078 Total 3892.00 This is an iBanking generated e-receipt and does not require any signature ```\n"
     ]
    }
   ],
   "source": [
    "context = get_context_from_embeddings(supabase, supabase_default_account_id, \"Does this mention balance?\", report_id=report_id)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aed56482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = get_context_from_embeddings(supabase, supabase_default_account_id, \"Earth Rotation?\", report_id=report_id)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8927e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Image\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import Tool\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7caeda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_medical_documents(query: str) -> str:\n",
    "    \"\"\"Useful when you need to look for information in the provided medical documents.\n",
    "    Input should be a fully formed question.\"\"\"\n",
    "    return get_context_from_embeddings(supabase, supabase_default_account_id, query, report_id=report_id) or \"No relevant information found in the documents.\"\n",
    "\n",
    "tool_search_docs = Tool(\n",
    "    name=\"search_medical_documents\",\n",
    "    description=\"Useful for when you need to look for information in the medical documents provided\"\n",
    "                \"The input to this tool should be a fully formed question.\",\n",
    "    func=search_medical_documents\n",
    ")\n",
    "\n",
    "tools = [tool_search_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e49b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = llm.bind_tools(tools=tools)\n",
    "system_prompt = \"\"\"\n",
    "    You are a medical assistant. Based on the user message, you will decide whether to response normally or to look for information\n",
    "    in the medical documents provided. When relevant be sure to look into the documents to provide accurate information. \n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def log_messages(messages: List[BaseMessage]) -> None:\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"Human: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"AI: {message.content}\")\n",
    "        elif isinstance(message, SystemMessage):\n",
    "            print(f\"System: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4ed9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advisor_node(old_state: State) -> State:\n",
    "    response = chain.invoke(old_state[\"messages\"])\n",
    "    print(\"________________________\")\n",
    "    print(log_messages(old_state[\"messages\"] + [response]))\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54226047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Work/Projects/AI-Engineer/medical-reports/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________\n",
      "Human: Hey\n",
      "AI: Hello! How can I assist you today?\n",
      "None\n",
      "________________________\n",
      "Human: Hey\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: what can you do\n",
      "AI: I can help you with a variety of tasks related to medical information and assistance. Some examples include:\n",
      "\n",
      "1. Answering general health questions.\n",
      "2. Providing information about medical terms and conditions.\n",
      "3. Looking up specific medical documents or guidelines for accurate information.\n",
      "4. Offering tips on wellness and preventative care.\n",
      "\n",
      "Let me know if you have a specific question or topic in mind!\n",
      "None\n",
      "________________________\n",
      "Human: Hey\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: what can you do\n",
      "AI: I can help you with a variety of tasks related to medical information and assistance. Some examples include:\n",
      "\n",
      "1. Answering general health questions.\n",
      "2. Providing information about medical terms and conditions.\n",
      "3. Looking up specific medical documents or guidelines for accurate information.\n",
      "4. Offering tips on wellness and preventative care.\n",
      "\n",
      "Let me know if you have a specific question or topic in mind!\n",
      "Human: can you share something about my files? anything that might be interesting\n",
      "AI: \n",
      "None\n",
      "________________________\n",
      "Human: Hey\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: what can you do\n",
      "AI: I can help you with a variety of tasks related to medical information and assistance. Some examples include:\n",
      "\n",
      "1. Answering general health questions.\n",
      "2. Providing information about medical terms and conditions.\n",
      "3. Looking up specific medical documents or guidelines for accurate information.\n",
      "4. Offering tips on wellness and preventative care.\n",
      "\n",
      "Let me know if you have a specific question or topic in mind!\n",
      "Human: can you share something about my files? anything that might be interesting\n",
      "AI: \n",
      "AI: It seems that the information retrieved from the files is related to a transaction receipt, specifically an online banking transaction detailing an amount of BDT 3892. If you are looking for something specific or more interesting related to medical information or health topics, please let me know!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"advisor\", advisor_node)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "\n",
    "graph_builder.add_edge(START, \"advisor\")\n",
    "graph_builder.add_conditional_edges(\"advisor\", tools_condition, \"tools\")\n",
    "graph_builder.add_edge(\"tools\", \"advisor\")\n",
    "graph_builder.add_edge(\"advisor\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": make_thread_id()}}\n",
    "\n",
    "def chat(user_message, history):\n",
    "    out = graph.invoke({\"messages\": [HumanMessage(content=user_message)]}, config=config)\n",
    "    return out[\"messages\"][-1].content\n",
    "\n",
    "gr.ChatInterface(fn=chat, title=\"LangGraph LLM Test\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdf and jpg image upload done\n",
    "### make sure any kind of image can be uploaded\n",
    "### refactor to reduce code duplication\n",
    "### connect database supabase with vectorstore. Compare Supabase and Vector DBs (FAISS, Pinecone) + SQL\n",
    "### double check industry standard\n",
    "### OCR of pdf and images\n",
    "### connect vector store\n",
    "### connect langgraph\n",
    "# convert to .py industry standard\n",
    "# create front end (react)\n",
    "# connect with python in an industry standard way (check FastAPI)\n",
    "# DONE!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ownership-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
